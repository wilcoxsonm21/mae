# Default configuration for autoencoder training

# Experiment settings
experiment_name: "default_experiment"
seed: 42
device: "cuda"  # or "cpu"

# Model configuration
model:
  type: "mlp_ae"  # Options: "mlp_ae", "mlp_mae"
  params:
    # input_dim is automatically set from dataset
    latent_dim: 32
    hidden_dims: [512, 256, 128]
    activation: "relu"  # Options: "relu", "tanh", "sigmoid"
    dropout: 0.0

# Training objective
objective:
  type: "reconstruction"  # Options: "reconstruction", "masked_reconstruction"
  params:
    loss_type: "mse"  # Options: "mse", "l1", "smooth_l1"
    reduction: "mean"  # Options: "mean", "sum"

# Dataset configuration
dataset:
  dataset_name: "gaussian_mixture"  # Options: "gaussian_mixture", "swiss_roll", "s_curve", "concentric_circles", "uniform"
  n_samples: 10000
  input_dim: 64
  train_split: 0.8
  batch_size: 128
  normalize: true
  random_state: 42
  # Additional dataset-specific params
  n_components: 5  # for gaussian_mixture
  # n_circles: 3  # for concentric_circles
  # noise: 0.1  # for swiss_roll, s_curve

# Optimizer configuration
optimizer:
  type: "adam"  # Options: "adam", "sgd"
  lr: 0.001
  weight_decay: 0.0
  # momentum: 0.9  # for sgd

# Training configuration
num_epochs: 100
checkpoint_dir: "./checkpoints"
visualization_frequency: 10  # Log visualizations every N epochs

# Weights & Biases configuration
wandb_project: "autoencoder-training"
