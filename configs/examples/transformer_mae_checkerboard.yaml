# Configuration for Transformer Masked Autoencoder on 2D checkerboard patterns
# Masked Autoencoder (MAE) architecture with patch-based masking

experiment_name: "transformer_mae_checkerboard"
seed: 42
device: "cuda"

# Transformer Masked Autoencoder
model:
  type: "transformer_mae"
  params:
    latent_dim: 128
    image_size: 32  # 32x32 images
    in_channels: 1  # Grayscale images
    patch_size: 4  # 4x4 patches (64 total patches)
    embed_dim: 384  # Embedding dimension
    num_heads: 6  # Number of attention heads
    encoder_depth: 6  # Number of encoder transformer blocks
    decoder_depth: 3  # Number of decoder transformer blocks (half of encoder)
    mlp_ratio: 4  # MLP hidden dimension = embed_dim * mlp_ratio
    dropout: 0.1
    mask_ratio: 0.75  # Mask 75% of patches (standard MAE setting)

# Masked reconstruction loss (only on masked patches)
objective:
  type: "masked_reconstruction"
  params:
    loss_type: "mse"
    reduction: "mean"
    predict_all: false  # Only compute loss on masked positions

# Checkerboard dataset with homographic transformations
dataset:
  dataset_name: "checkerboard"
  n_samples: 20000
  image_size: 32  # 32x32 images (input_dim = 1024)
  train_split: 0.8
  batch_size: 128  # May need to reduce to 64 if memory issues
  normalize: true
  random_state: 42
  # Checkerboard parameters
  grid_sizes: [2, 4, 8, 16]
  noise_level: 0.01
  # Homographic transformation parameters
  apply_transforms: true
  rotation_range: 15.0  # Â±15 degrees
  scale_range: [0.8, 1.2]  # 80% to 120% scaling
  perspective_range: 0.2  # Amount of perspective distortion
  # Save generation parameters for downstream evaluation
  return_params: true

# Adam optimizer with higher learning rate for MAE
optimizer:
  type: "adam"
  lr: 0.0003  # Higher LR for MAE (like CNN MAE)
  weight_decay: 0.0

# Training settings - transformers may need more epochs
num_epochs: 100  # Increased from 20 for transformer
checkpoint_dir: "./checkpoints/transformer_mae_checkerboard"
visualization_frequency: 10  # Log visualizations every N epochs

# Wandb
wandb_project: "autoencoder-training"

# Downstream evaluation settings
downstream:
  enabled: true
  train_probes: true
  probe_config:
    probe_type: "mlp"  # Use MLP probe (default)
    hidden_dim: 64
    lr: 0.001
    weight_decay: 0.0001
    epochs: 100
    batch_size: 256
    patience: 15
