# Example configuration for standard MLP Autoencoder on synthetic Gaussian mixture data

experiment_name: "mlp_ae_gaussian_mixture"
seed: 42
device: "cuda"

# Standard MLP Autoencoder
model:
  type: "mlp_ae"
  params:
    latent_dim: 32
    hidden_dims: [512, 256, 128]
    activation: "relu"
    dropout: 0.1

# Standard reconstruction loss
objective:
  type: "reconstruction"
  params:
    loss_type: "mse"
    reduction: "mean"

# Gaussian mixture dataset
dataset:
  dataset_name: "gaussian_mixture"
  n_samples: 20000
  input_dim: 64
  train_split: 0.8
  batch_size: 128
  normalize: true
  random_state: 42
  n_components: 8

# Adam optimizer
optimizer:
  type: "adam"
  lr: 0.001
  weight_decay: 0.00001

# Training settings
num_epochs: 100
checkpoint_dir: "./checkpoints/ae_gaussian"
visualization_frequency: 10  # Log visualizations every N epochs

# Wandb
wandb_project: "autoencoder-training"
