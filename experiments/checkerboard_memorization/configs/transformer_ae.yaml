# Configuration for Transformer Autoencoder on checkerboard_memorization dataset
# Tests memorization vs generalization with 16 grid size classes and translation only

experiment_name: "transformer_ae_checkerboard_memorization"
seed: 42
device: "cuda"

# Transformer Autoencoder
model:
  type: "transformer_ae"
  params:
    latent_dim: 128
    image_size: 32
    in_channels: 1
    patch_size: 4  # 4x4 patches (64 total patches)
    embed_dim: 384
    num_heads: 6
    encoder_depth: 6
    decoder_depth: 3
    mlp_ratio: 4
    dropout: 0.1

# Standard reconstruction loss
objective:
  type: "reconstruction"
  params:
    loss_type: "mse"
    reduction: "mean"

# Checkerboard memorization dataset
dataset:
  dataset_name: "checkerboard_memorization"
  n_samples: 50  # Small dataset for overfitting test
  image_size: 32
  train_split: 0.8
  batch_size: 64
  normalize: true
  random_state: 42
  # Grid sizes: 2, 4, 8, 16 (4 classes) - uses default
  # Translation: up to Â±(cell_size - 1) pixels with rolling
  noise_level: 0.01
  return_params: true

# Adam optimizer
optimizer:
  type: "adam"
  lr: 0.0003
  weight_decay: 0.00001

# Training settings
num_epochs: 100
checkpoint_dir: "./checkpoints/checkerboard_memorization/transformer_ae"
visualization_frequency: 10

# Wandb
wandb_project: "autoencoder-training"

# Downstream evaluation settings
downstream:
  enabled: true
  train_probes: true
  probe_config:
    probe_type: "mlp"
    hidden_dim: 64
    lr: 0.001
    weight_decay: 0.0001
    epochs: 100
    batch_size: 256
    patience: 15
