experiment_name: transformer_ae_sweep_n50
seed: 42
device: cuda
model:
  type: transformer_ae
  params:
    latent_dim: 128
    image_size: 32
    in_channels: 1
    patch_size: 4
    embed_dim: 384
    num_heads: 6
    encoder_depth: 6
    decoder_depth: 3
    mlp_ratio: 4
    dropout: 0.1
objective:
  type: reconstruction
  params:
    loss_type: mse
    reduction: mean
dataset:
  dataset_name: checkerboard_memorization
  n_samples: 50
  image_size: 32
  train_split: 0.8
  batch_size: 64
  normalize: true
  random_state: 42
  noise_level: 0.01
  return_params: true
optimizer:
  type: adam
  lr: 0.0003
  weight_decay: 1.0e-05
num_epochs: 100
checkpoint_dir: ./checkpoints/checkerboard_memorization/sweep/transformer_ae_n50
visualization_frequency: 10
wandb_project: autoencoder-training
downstream:
  enabled: true
  train_probes: true
  probe_config:
    probe_type: mlp
    hidden_dim: 64
    lr: 0.001
    weight_decay: 0.0001
    epochs: 100
    batch_size: 256
    patience: 15
early_stop_overfit:
  enabled: true
  patience: 10
